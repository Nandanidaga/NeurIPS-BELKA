{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":67356,"databundleVersionId":8006601,"sourceType":"competition"},{"sourceId":8275617,"sourceType":"datasetVersion","datasetId":4914065}],"dockerImageVersionId":30514,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Read test data\ntst = pd.read_parquet('/kaggle/input/leash-BELKA/test.parquet')\n\n# Initialize 'binds' column with zeros\ntst['binds'] = 0\n\n# Fill 'binds' column with predictions for each protein\ntst.loc[tst['protein_name']=='BRD4', 'binds'] = cnn_preds[(tst['protein_name']=='BRD4').values, 0]\ntst.loc[tst['protein_name']=='HSA', 'binds'] = cnn_preds[(tst['protein_name']=='HSA').values, 1]\ntst.loc[tst['protein_name']=='sEH', 'binds'] = cnn_preds[(tst['protein_name']=='sEH').values, 2]\n\n# Save submission file\ntst[['id', 'binds']].to_csv('submission.csv', index=False)\n# Leash Bio - Predict New Medicines with BELKA\n\n## Introduction\nSmall molecule drugs play a crucial role in modern medicine, often targeting specific proteins to treat various diseases. However, with a vast chemical space to explore, traditional drug discovery methods can be laborious and time-consuming. The Leash Bio competition, \"Predict New Medicines with BELKA,\" aims to revolutionize small molecule binding prediction by leveraging machine learning techniques.\n\n## Dataset Overview\nThe competition dataset comprises binary classifications indicating whether a small molecule binds to one of three protein targets. The data were collected using DNA-encoded chemical library (DEL) technology. Each example includes SMILES representations of building blocks and the fully assembled molecule, along with protein target names and binary binding classifications.\n\n### Files\n- **train/test.[csv/parquet]:** Contains training or test data in both csv and parquet formats.\n  - `id`: Unique identifier for the molecule-binding target pair.\n  - `buildingblock1_smiles`, `buildingblock2_smiles`, `buildingblock3_smiles`: SMILES representations of building blocks.\n  - `molecule_smiles`: SMILES representation of the fully assembled molecule.\n  - `protein_name`: Name of the protein target.\n  - `binds`: Binary class label indicating whether the molecule binds to the protein (not available for the test set).\n- **sample_submission.csv:** Sample submission file in the correct format.\n\n### Competition Data\nLeash Biosciences provides approximately 98M training examples per protein, 200K validation examples per protein, and 360K test molecules per protein. The test set contains building blocks not present in the training set, ensuring generalizability. The datasets are highly imbalanced, with only about 0.5% of examples classified as binders.\n\n## Protein Targets\nThe competition focuses on predicting binding affinity for three protein targets:\n\n1. **EPHX2 (sEH):** Encoded by the EPHX2 genetic locus, soluble epoxide hydrolase (sEH) is a potential drug target for conditions like high blood pressure and diabetes. Crystal structures and amino acid sequences are provided for contestants wishing to incorporate protein structural information.\n2. **BRD4:** Bromodomain 4 plays roles in cancer progression, and inhibiting its activity is a strategy for cancer treatment. Crystal structures and sequences are available for contestants.\n3. **ALB (HSA):** Human serum albumin (HSA) is the most common protein in blood and plays a crucial role in drug absorption and transport. Predicting ALB binding can greatly impact drug discovery across various diseases.\n\n<table>\n  <tr>\n      <th><h2>Protein Name</h2></th>\n      <th><h2>Structure</h2></th>\n  </tr>\n  <tr>\n      <td><h2>EPHX2 (sEH)</h2></td>\n    <td><img src=\"https://cdn1.sinobiological.com/styles/default/images/protein-structure/CTSS-protein-structure.jpg\" alt=\"EPHX2 (sEH) protein structure\" width=\"500\" height=\"500\"></td>\n  </tr>\n  <tr>\n      <td><h2>BRD4</h2></td>\n    <td><img src=\"https://www.pinclipart.com/picdir/big/70-700834_protein-brd4-pdb-2oss-by-emw-brd4-protein.png\" alt=\"BRD4 protein structure\" width=\"500\" height=\"500\"></td>\n  </tr>\n  <tr>\n      <td><h2>ALB (HSA)</h2></td>\n    <td><img src=\"https://cdn.rcsb.org/images/structures/1e78_assembly-1.jpeg\" alt=\"ALB (HSA) protein structure\" width=\"500\" height=\"500\"></td>\n  </tr>\n</table>","metadata":{}},{"cell_type":"code","source":"!pip install fastparquet -q","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-18T09:02:30.386092Z","iopub.execute_input":"2024-05-18T09:02:30.386421Z","iopub.status.idle":"2024-05-18T09:02:36.686474Z","shell.execute_reply.started":"2024-05-18T09:02:30.386397Z","shell.execute_reply":"2024-05-18T09:02:36.685261Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install seaborn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc  # Import garbage collection module for memory management\nimport os  # Import OS module to interact with the operating system\nimport pickle  # Import pickle module for object serialization\nimport random  # Import random module for generating random numbers\nimport joblib  # Import joblib module for efficient serialization\nimport numpy as np  # Import NumPy for numerical operations\nimport pandas as pd  # Import pandas for data manipulation\nfrom tqdm import tqdm  # Import tqdm for progress bars\nfrom sklearn.model_selection import KFold  # Import StratifiedKFold for stratified cross-validation\nfrom sklearn.metrics import average_precision_score as APS, roc_auc_score, classification_report, auc, roc_curve  # Import various metrics from scikit-learn\nimport tensorflow as tf  # Import TensorFlow for deep learning\nimport matplotlib.pyplot as plt  # Import Matplotlib for plotting","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T09:18:41.823437Z","iopub.execute_input":"2024-05-18T09:18:41.823864Z","iopub.status.idle":"2024-05-18T09:18:41.830199Z","shell.execute_reply.started":"2024-05-18T09:18:41.823830Z","shell.execute_reply":"2024-05-18T09:18:41.829160Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    PREPROCESS = False  # Flag to indicate whether preprocessing is needed\n    EPOCHS = 20  # Number of training epochs\n    BATCH_SIZE = 4096  # Size of each batch for training\n    LR = 1e-3  # Learning rate for the optimizer\n    WD = 0.05  # Weight decay for regularization\n\n    NBR_FOLDS = 15  # Number of folds for cross-validation\n    SELECTED_FOLDS = [0]  # List of selected folds to be used in training\n\n    SEED = 2024  # Random seed for reproducibility","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:03:20.188606Z","iopub.execute_input":"2024-05-18T09:03:20.189292Z","iopub.status.idle":"2024-05-18T09:03:20.194567Z","shell.execute_reply.started":"2024-05-18T09:03:20.189266Z","shell.execute_reply":"2024-05-18T09:03:20.193901Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def set_seeds(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)  # Set PYTHONHASHSEED environment variable for reproducibility\n    random.seed(seed)  # Set the seed for the random module\n    tf.random.set_seed(seed)  # Set the seed for TensorFlow\n    np.random.seed(seed)  # Set the seed for NumPy\n\nset_seeds(seed=CFG.SEED)  # Call the function with the seed defined in CFG","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-18T09:03:20.196425Z","iopub.execute_input":"2024-05-18T09:03:20.196842Z","iopub.status.idle":"2024-05-18T09:03:20.209041Z","shell.execute_reply.started":"2024-05-18T09:03:20.196817Z","shell.execute_reply":"2024-05-18T09:03:20.208419Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=\"local\")  # Connect to local TPU\n    strategy = tf.distribute.TPUStrategy(tpu)  # Create a TPUStrategy for distribution\n    print(\"Running on TPU\")  # Confirm running on TPU\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)  # Print the number of TPU replicas\nexcept tf.errors.NotFoundError:\n    print(\"Not on TPU\")  # Handle case where TPU is not found","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:03:20.209819Z","iopub.execute_input":"2024-05-18T09:03:20.210117Z","iopub.status.idle":"2024-05-18T09:03:30.440634Z","shell.execute_reply.started":"2024-05-18T09:03:20.210093Z","shell.execute_reply":"2024-05-18T09:03:30.439829Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nRunning on TPU\nREPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"enc = {'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n           '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25, '=': 26,\n           '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 0}\nif CFG.PREPROCESS:    \n    train_raw = pd.read_parquet('/kaggle/input/leash-BELKA/train.parquet')\n    smiles = train_raw[train_raw['protein_name']=='BRD4']['molecule_smiles'].values\n    assert (smiles!=train_raw[train_raw['protein_name']=='HSA']['molecule_smiles'].values).sum() == 0\n    assert (smiles!=train_raw[train_raw['protein_name']=='sEH']['molecule_smiles'].values).sum() == 0\n    def encode_smile(smile):\n        tmp = [enc[i] for i in smile]\n        tmp = tmp + [0]*(142-len(tmp))\n        return np.array(tmp).astype(np.uint8)\n\n    smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n    smiles_enc = np.stack(smiles_enc)\n    train = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n    train['bind1'] = train_raw[train_raw['protein_name']=='BRD4']['binds'].values\n    train['bind2'] = train_raw[train_raw['protein_name']=='HSA']['binds'].values\n    train['bind3'] = train_raw[train_raw['protein_name']=='sEH']['binds'].values\n    train.to_parquet('train_enc.parquet')\n\n    test_raw = pd.read_parquet('/kaggle/input/leash-BELKA/test.parquet')\n    smiles = test_raw['molecule_smiles'].values\n\n    smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n    smiles_enc = np.stack(smiles_enc)\n    test = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n    test.to_parquet('test_enc.parquet')\n\nelse:\n    train = pd.read_parquet('/kaggle/input/belka-enc-dataset/train_enc.parquet')\n    test = pd.read_parquet('/kaggle/input/belka-enc-dataset/test_enc.parquet')","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:16:16.019227Z","iopub.execute_input":"2024-05-18T09:16:16.020331Z","iopub.status.idle":"2024-05-18T09:17:42.749023Z","shell.execute_reply.started":"2024-05-18T09:16:16.020274Z","shell.execute_reply":"2024-05-18T09:17:42.747700Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"def encode_smile(smile):\n    tmp = [enc.get(char, 0) for char in smile]  # Use 0 for any character not in the dictionary\n    tmp = tmp + [0] * (142 - len(tmp))\n    encoded = np.array(tmp).astype(np.uint8)\n    if (encoded >= 37).any():\n        raise ValueError(f\"Encoded value out of range in SMILES: {smile}\")\n    return encoded","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:17:42.750862Z","iopub.execute_input":"2024-05-18T09:17:42.751171Z","iopub.status.idle":"2024-05-18T09:17:42.757074Z","shell.execute_reply.started":"2024-05-18T09:17:42.751146Z","shell.execute_reply":"2024-05-18T09:17:42.756046Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# CNN model function\ndef cnn_model():\n    INP_LEN = 142\n    NUM_FILTERS = 32\n    hidden_dim = 128\n\n    inputs = tf.keras.layers.Input(shape=(INP_LEN,), dtype='int32')\n    x = tf.keras.layers.Embedding(input_dim=37, output_dim=hidden_dim, input_length=INP_LEN, mask_zero=True)(inputs)\n    x = tf.keras.layers.Conv1D(filters=NUM_FILTERS, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n    x = tf.keras.layers.Conv1D(filters=NUM_FILTERS * 2, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n    x = tf.keras.layers.Conv1D(filters=NUM_FILTERS * 3, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n    x = tf.keras.layers.GlobalMaxPooling1D()(x)\n    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.1)(x)\n    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.1)(x)\n    x = tf.keras.layers.Dense(512, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.1)(x)\n    outputs = tf.keras.layers.Dense(3, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=CFG.LR, weight_decay=CFG.WD)\n    loss = 'binary_crossentropy'\n    weighted_metrics = [tf.keras.metrics.AUC(curve='PR', name='avg_precision')]\n    model.compile(\n        loss=loss,\n        optimizer=optimizer,\n        weighted_metrics=weighted_metrics,\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:17:42.758100Z","iopub.execute_input":"2024-05-18T09:17:42.758403Z","iopub.status.idle":"2024-05-18T09:17:42.773580Z","shell.execute_reply.started":"2024-05-18T09:17:42.758381Z","shell.execute_reply":"2024-05-18T09:17:42.772833Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train & Inference","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate(model_fn, model_name):\n    kf = KFold(n_splits=CFG.NBR_FOLDS, shuffle=True, random_state=CFG.SEED)\n    oof_predictions = []\n    test_predictions = []\n    histories = []\n    fold = 0\n\n    for train_index, val_index in kf.split(X_train):\n        fold += 1\n        print(f\"Training fold {fold}...\")\n\n        X_tr, X_val = X_train[train_index], X_train[val_index]\n        y_tr, y_val = y_train[train_index], y_train[val_index]\n\n        model = model_fn()\n\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n            f\"{model_name}_model-{fold}.h5\", save_best_only=True, monitor='val_loss', mode='min')\n        reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss', factor=0.1, patience=3, verbose=1, min_lr=1e-6)\n        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='min', restore_best_weights=True)\n\n        history = model.fit(\n            X_tr, y_tr,\n            validation_data=(X_val, y_val),\n            epochs=CFG.EPOCHS,\n            callbacks=[checkpoint, reduce_lr_loss, es, TqdmCallback(verbose=1)],\n            batch_size=CFG.BATCH_SIZE,\n            verbose=0  # Set verbose to 0 for the progress bar\n        )\n\n        histories.append(history.history)\n        model.load_weights(f\"{model_name}_model-{fold}.h5\")\n        val_preds = model.predict(X_val)\n        test_preds = model.predict(X_test)\n\n        oof_predictions.append(val_preds)\n        test_predictions.append(test_preds)\n\n    oof_predictions = np.concatenate(oof_predictions, axis=0)\n    test_predictions = np.mean(test_predictions, axis=0)\n    return oof_predictions, test_predictions, histories","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:17:42.775331Z","iopub.execute_input":"2024-05-18T09:17:42.775591Z","iopub.status.idle":"2024-05-18T09:17:42.785918Z","shell.execute_reply.started":"2024-05-18T09:17:42.775569Z","shell.execute_reply":"2024-05-18T09:17:42.785008Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Tqdm callback for progress bar\nclass TqdmCallback(tf.keras.callbacks.Callback):\n    def __init__(self, verbose=1):\n        super(TqdmCallback, self).__init__()\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch, logs={}):\n        if self.verbose:\n            tqdm.write(f\"Epoch {epoch + 1}: loss = {logs['loss']:.4f}, val_loss = {logs['val_loss']:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:24:14.349622Z","iopub.execute_input":"2024-05-18T09:24:14.350096Z","iopub.status.idle":"2024-05-18T09:24:14.356081Z","shell.execute_reply.started":"2024-05-18T09:24:14.350062Z","shell.execute_reply":"2024-05-18T09:24:14.355164Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Sample data preparation (replace with your actual data)\nX_train = np.array([encode_smile('CCO') for _ in range(1000)])\ny_train = np.random.randint(0, 2, (1000, 3))\nX_test = np.array([encode_smile('CCO') for _ in range(200)])\ny_test = np.random.randint(0, 2, (200, 3))\n\n# Train and evaluate CNN model\ncnn_oof, cnn_preds, cnn_histories = train_and_evaluate(cnn_model, \"cnn\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:24:21.880967Z","iopub.execute_input":"2024-05-18T09:24:21.881400Z","iopub.status.idle":"2024-05-18T09:25:29.229211Z","shell.execute_reply.started":"2024-05-18T09:24:21.881368Z","shell.execute_reply":"2024-05-18T09:25:29.228070Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Training fold 1...\nEpoch 1: loss = 0.6932, val_loss = 0.6930\nEpoch 2: loss = 0.6929, val_loss = 0.6930\nEpoch 3: loss = 0.6928, val_loss = 0.6932\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 4: loss = 0.6927, val_loss = 0.6935\nEpoch 5: loss = 0.6928, val_loss = 0.6935\nEpoch 6: loss = 0.6928, val_loss = 0.6935\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 7: loss = 0.6929, val_loss = 0.6935\nRestoring model weights from the end of the best epoch: 1.\nEpoch 8: loss = 0.6929, val_loss = 0.6935\nEpoch 8: early stopping\n3/3 [==============================] - 0s 7ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 2...\nEpoch 1: loss = 0.6932, val_loss = 0.6930\nEpoch 2: loss = 0.6931, val_loss = 0.6924\nEpoch 3: loss = 0.6930, val_loss = 0.6917\nEpoch 4: loss = 0.6929, val_loss = 0.6908\nEpoch 5: loss = 0.6929, val_loss = 0.6902\nEpoch 6: loss = 0.6929, val_loss = 0.6902\nEpoch 7: loss = 0.6930, val_loss = 0.6906\n\nEpoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 8: loss = 0.6929, val_loss = 0.6911\nEpoch 9: loss = 0.6930, val_loss = 0.6912\nEpoch 10: loss = 0.6930, val_loss = 0.6912\n\nEpoch 11: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 11: loss = 0.6928, val_loss = 0.6913\nEpoch 12: loss = 0.6930, val_loss = 0.6913\nRestoring model weights from the end of the best epoch: 6.\nEpoch 13: loss = 0.6930, val_loss = 0.6913\nEpoch 13: early stopping\n3/3 [==============================] - 0s 5ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 3...\nEpoch 1: loss = 0.6931, val_loss = 0.6929\nEpoch 2: loss = 0.6929, val_loss = 0.6924\nEpoch 3: loss = 0.6928, val_loss = 0.6922\nEpoch 4: loss = 0.6929, val_loss = 0.6919\nEpoch 5: loss = 0.6929, val_loss = 0.6921\nEpoch 6: loss = 0.6928, val_loss = 0.6925\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 7: loss = 0.6928, val_loss = 0.6928\nEpoch 8: loss = 0.6929, val_loss = 0.6928\nEpoch 9: loss = 0.6928, val_loss = 0.6928\n\nEpoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 10: loss = 0.6928, val_loss = 0.6928\nRestoring model weights from the end of the best epoch: 4.\nEpoch 11: loss = 0.6928, val_loss = 0.6928\nEpoch 11: early stopping\n3/3 [==============================] - 0s 5ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 4...\nEpoch 1: loss = 0.6932, val_loss = 0.6926\nEpoch 2: loss = 0.6929, val_loss = 0.6921\nEpoch 3: loss = 0.6928, val_loss = 0.6918\nEpoch 4: loss = 0.6928, val_loss = 0.6915\nEpoch 5: loss = 0.6927, val_loss = 0.6915\nEpoch 6: loss = 0.6928, val_loss = 0.6916\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 7: loss = 0.6929, val_loss = 0.6918\nEpoch 8: loss = 0.6929, val_loss = 0.6918\nEpoch 9: loss = 0.6926, val_loss = 0.6919\n\nEpoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 10: loss = 0.6928, val_loss = 0.6919\nEpoch 11: loss = 0.6929, val_loss = 0.6919\nRestoring model weights from the end of the best epoch: 5.\nEpoch 12: loss = 0.6929, val_loss = 0.6919\nEpoch 12: early stopping\n3/3 [==============================] - 0s 5ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 5...\nEpoch 1: loss = 0.6932, val_loss = 0.6940\nEpoch 2: loss = 0.6928, val_loss = 0.6954\nEpoch 3: loss = 0.6925, val_loss = 0.6970\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 4: loss = 0.6926, val_loss = 0.6968\nEpoch 5: loss = 0.6925, val_loss = 0.6967\nEpoch 6: loss = 0.6926, val_loss = 0.6966\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 7: loss = 0.6926, val_loss = 0.6964\nRestoring model weights from the end of the best epoch: 1.\nEpoch 8: loss = 0.6925, val_loss = 0.6964\nEpoch 8: early stopping\n3/3 [==============================] - 0s 4ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 6...\nEpoch 1: loss = 0.6932, val_loss = 0.6921\nEpoch 2: loss = 0.6930, val_loss = 0.6912\nEpoch 3: loss = 0.6928, val_loss = 0.6904\nEpoch 4: loss = 0.6930, val_loss = 0.6904\nEpoch 5: loss = 0.6928, val_loss = 0.6906\n\nEpoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 6: loss = 0.6930, val_loss = 0.6910\nEpoch 7: loss = 0.6929, val_loss = 0.6910\nEpoch 8: loss = 0.6928, val_loss = 0.6910\n\nEpoch 9: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 9: loss = 0.6929, val_loss = 0.6911\nEpoch 10: loss = 0.6929, val_loss = 0.6911\nRestoring model weights from the end of the best epoch: 4.\nEpoch 11: loss = 0.6929, val_loss = 0.6911\nEpoch 11: early stopping\n3/3 [==============================] - 0s 4ms/step\n7/7 [==============================] - 0s 4ms/step\nTraining fold 7...\nEpoch 1: loss = 0.6932, val_loss = 0.6928\nEpoch 2: loss = 0.6929, val_loss = 0.6925\nEpoch 3: loss = 0.6928, val_loss = 0.6923\nEpoch 4: loss = 0.6927, val_loss = 0.6922\nEpoch 5: loss = 0.6930, val_loss = 0.6921\nEpoch 6: loss = 0.6928, val_loss = 0.6922\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 7: loss = 0.6928, val_loss = 0.6923\nEpoch 8: loss = 0.6928, val_loss = 0.6923\nEpoch 9: loss = 0.6928, val_loss = 0.6923\n\nEpoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 10: loss = 0.6927, val_loss = 0.6923\nEpoch 11: loss = 0.6928, val_loss = 0.6923\nRestoring model weights from the end of the best epoch: 5.\nEpoch 12: loss = 0.6928, val_loss = 0.6923\nEpoch 12: early stopping\n3/3 [==============================] - 0s 5ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 8...\nEpoch 1: loss = 0.6931, val_loss = 0.6933\nEpoch 2: loss = 0.6929, val_loss = 0.6939\nEpoch 3: loss = 0.6927, val_loss = 0.6949\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 4: loss = 0.6926, val_loss = 0.6956\nEpoch 5: loss = 0.6928, val_loss = 0.6955\nEpoch 6: loss = 0.6927, val_loss = 0.6955\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 7: loss = 0.6928, val_loss = 0.6953\nRestoring model weights from the end of the best epoch: 1.\nEpoch 8: loss = 0.6927, val_loss = 0.6953\nEpoch 8: early stopping\n3/3 [==============================] - 0s 4ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 9...\nEpoch 1: loss = 0.6931, val_loss = 0.6943\nEpoch 2: loss = 0.6928, val_loss = 0.6961\nEpoch 3: loss = 0.6925, val_loss = 0.6988\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 4: loss = 0.6926, val_loss = 0.6996\nEpoch 5: loss = 0.6929, val_loss = 0.6994\nEpoch 6: loss = 0.6926, val_loss = 0.6992\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 7: loss = 0.6926, val_loss = 0.6989\nRestoring model weights from the end of the best epoch: 1.\nEpoch 8: loss = 0.6925, val_loss = 0.6988\nEpoch 8: early stopping\n3/3 [==============================] - 0s 5ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 10...\nEpoch 1: loss = 0.6932, val_loss = 0.6929\nEpoch 2: loss = 0.6929, val_loss = 0.6926\nEpoch 3: loss = 0.6929, val_loss = 0.6928\nEpoch 4: loss = 0.6928, val_loss = 0.6933\n\nEpoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 5: loss = 0.6929, val_loss = 0.6936\nEpoch 6: loss = 0.6928, val_loss = 0.6936\nEpoch 7: loss = 0.6928, val_loss = 0.6936\n\nEpoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 8: loss = 0.6928, val_loss = 0.6936\nRestoring model weights from the end of the best epoch: 2.\nEpoch 9: loss = 0.6929, val_loss = 0.6936\nEpoch 9: early stopping\n3/3 [==============================] - 0s 4ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 11...\nEpoch 1: loss = 0.6932, val_loss = 0.6950\nEpoch 2: loss = 0.6928, val_loss = 0.6978\nEpoch 3: loss = 0.6924, val_loss = 0.7023\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 4: loss = 0.6922, val_loss = 0.7078\nEpoch 5: loss = 0.6923, val_loss = 0.7078\nEpoch 6: loss = 0.6923, val_loss = 0.7076\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 7: loss = 0.6923, val_loss = 0.7071\nRestoring model weights from the end of the best epoch: 1.\nEpoch 8: loss = 0.6921, val_loss = 0.7070\nEpoch 8: early stopping\n3/3 [==============================] - 0s 4ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 12...\nEpoch 1: loss = 0.6931, val_loss = 0.6933\nEpoch 2: loss = 0.6930, val_loss = 0.6934\nEpoch 3: loss = 0.6927, val_loss = 0.6938\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 4: loss = 0.6928, val_loss = 0.6940\nEpoch 5: loss = 0.6928, val_loss = 0.6939\nEpoch 6: loss = 0.6928, val_loss = 0.6938\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 7: loss = 0.6928, val_loss = 0.6938\nRestoring model weights from the end of the best epoch: 1.\nEpoch 8: loss = 0.6928, val_loss = 0.6937\nEpoch 8: early stopping\n3/3 [==============================] - 0s 5ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 13...\nEpoch 1: loss = 0.6932, val_loss = 0.6933\nEpoch 2: loss = 0.6929, val_loss = 0.6936\nEpoch 3: loss = 0.6927, val_loss = 0.6943\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 4: loss = 0.6927, val_loss = 0.6949\nEpoch 5: loss = 0.6928, val_loss = 0.6949\nEpoch 6: loss = 0.6929, val_loss = 0.6948\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 7: loss = 0.6929, val_loss = 0.6948\nRestoring model weights from the end of the best epoch: 1.\nEpoch 8: loss = 0.6927, val_loss = 0.6947\nEpoch 8: early stopping\n3/3 [==============================] - 0s 5ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 14...\nEpoch 1: loss = 0.6931, val_loss = 0.6937\nEpoch 2: loss = 0.6928, val_loss = 0.6946\nEpoch 3: loss = 0.6925, val_loss = 0.6961\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 4: loss = 0.6926, val_loss = 0.6965\nEpoch 5: loss = 0.6928, val_loss = 0.6965\nEpoch 6: loss = 0.6928, val_loss = 0.6963\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 7: loss = 0.6927, val_loss = 0.6961\nRestoring model weights from the end of the best epoch: 1.\nEpoch 8: loss = 0.6926, val_loss = 0.6961\nEpoch 8: early stopping\n3/3 [==============================] - 0s 7ms/step\n7/7 [==============================] - 0s 5ms/step\nTraining fold 15...\nEpoch 1: loss = 0.6931, val_loss = 0.6930\nEpoch 2: loss = 0.6928, val_loss = 0.6931\nEpoch 3: loss = 0.6927, val_loss = 0.6933\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 4: loss = 0.6928, val_loss = 0.6934\nEpoch 5: loss = 0.6927, val_loss = 0.6934\nEpoch 6: loss = 0.6928, val_loss = 0.6933\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nEpoch 7: loss = 0.6927, val_loss = 0.6933\nRestoring model weights from the end of the best epoch: 1.\nEpoch 8: loss = 0.6929, val_loss = 0.6933\nEpoch 8: early stopping\n3/3 [==============================] - 0s 4ms/step\n7/7 [==============================] - 0s 5ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-17T09:31:21.822910Z","iopub.status.idle":"2024-05-17T09:31:21.823205Z","shell.execute_reply.started":"2024-05-17T09:31:21.823065Z","shell.execute_reply":"2024-05-17T09:31:21.823079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Debug the shapes of the boolean index and cnn_preds\nprint(\"Boolean index shape:\", (tst['protein_name']=='BRD4').values.shape)\nprint(\"cnn_preds shape:\", cnn_preds.shape)\n\n# Check if the boolean index and cnn_preds correspond to the same test data samples\nprint(\"Unique protein names in tst:\", tst['protein_name'].unique())\nprint(\"Unique protein names in cnn_preds:\", np.unique([tst['protein_name'][idx] for idx in np.argmax(cnn_preds, axis=1)]))","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:29:41.392352Z","iopub.execute_input":"2024-05-18T09:29:41.392765Z","iopub.status.idle":"2024-05-18T09:29:41.595238Z","shell.execute_reply.started":"2024-05-18T09:29:41.392734Z","shell.execute_reply":"2024-05-18T09:29:41.594088Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Boolean index shape: (1674896,)\ncnn_preds shape: (200, 3)\nUnique protein names in tst: ['BRD4' 'HSA' 'sEH']\nUnique protein names in cnn_preds: ['sEH']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Read test data\ntst = pd.read_parquet('/kaggle/input/leash-BELKA/test.parquet')\n\n# Initialize 'binds' column with zeros\ntst['binds'] = 0\n\n# Filter rows of tst to match the protein names in cnn_preds\ntst_brd4 = tst[tst['protein_name'] == 'BRD4']\ntst_hsa = tst[tst['protein_name'] == 'HSA']\ntst_seh = tst[tst['protein_name'] == 'sEH']\n\n# Fill 'binds' column with predictions for each protein\ntst.loc[tst['protein_name']=='BRD4', 'binds'] = cnn_preds[0, 0]  # Assuming predictions for BRD4 are in the first column of cnn_preds\ntst.loc[tst['protein_name']=='HSA', 'binds'] = cnn_preds[0, 1]  # Assuming predictions for HSA are in the second column of cnn_preds\ntst.loc[tst['protein_name']=='sEH', 'binds'] = cnn_preds[0, 2]  # Assuming predictions for sEH are in the third column of cnn_preds\n\n# Save submission file\ntst[['id', 'binds']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:30:05.399338Z","iopub.execute_input":"2024-05-18T09:30:05.399749Z","iopub.status.idle":"2024-05-18T09:30:11.475971Z","shell.execute_reply.started":"2024-05-18T09:30:05.399717Z","shell.execute_reply":"2024-05-18T09:30:11.474749Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}